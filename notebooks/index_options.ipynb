{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b86d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2025-09-29 already exists. Use force_update=True to overwrite.\n",
      "\n",
      "============================================================\n",
      "To set up automated daily updates:\n",
      "1. Save the daily_update() function as 'update_vol_surfaces.py'\n",
      "2. Open Windows Task Scheduler\n",
      "3. Create Basic Task > Daily > Set time (e.g., 8:00 AM)\n",
      "4. Action: Start a program\n",
      "5. Program: python.exe\n",
      "6. Arguments: update_vol_surfaces.py\n",
      "7. Start in: [directory of the script]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "class VolSurfaceDatabase:\n",
    "    \"\"\"\n",
    "    Vol Surface Database Manager for Credit Index Options\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path=None):\n",
    "        \"\"\"Initialize database connection and logging\"\"\"\n",
    "        if db_path is None:\n",
    "            self.db_path = r\"C:\\source\\repos\\psc\\packages\\psc_csa_tools\\credit_macro\\data\\raw\\vol_surfaces.db\"\n",
    "        else:\n",
    "            self.db_path = db_path\n",
    "        \n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        \n",
    "        # Setup logging\n",
    "        log_path = os.path.join(os.path.dirname(self.db_path), \"vol_surface_updates.log\")\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_path),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def check_if_date_exists(self, data_date):\n",
    "        \"\"\"Check if data for specific date already exists\"\"\"\n",
    "        if not os.path.exists(self.db_path):\n",
    "            return False\n",
    "            \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Check if table exists first\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT name FROM sqlite_master \n",
    "            WHERE type='table' AND name='vol_surfaces'\n",
    "        \"\"\")\n",
    "        \n",
    "        if not cursor.fetchone():\n",
    "            conn.close()\n",
    "            return False\n",
    "        \n",
    "        cursor.execute('''\n",
    "            SELECT COUNT(*) FROM vol_surfaces \n",
    "            WHERE data_date = ?\n",
    "        ''', (data_date,))\n",
    "        \n",
    "        count = cursor.fetchone()[0]\n",
    "        conn.close()\n",
    "        \n",
    "        return count > 0\n",
    "    \n",
    "    def create_or_update_database(self, excel_path, data_date=None, force_update=False):\n",
    "        \"\"\"\n",
    "        Create or update the vol surface database from Excel\n",
    "        \"\"\"\n",
    "        if not os.path.exists(excel_path):\n",
    "            self.logger.error(f\"Excel file not found: {excel_path}\")\n",
    "            return 0\n",
    "        \n",
    "        if data_date is None:\n",
    "            data_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        # Check if data already exists\n",
    "        if not force_update and self.check_if_date_exists(data_date):\n",
    "            self.logger.info(f\"Data for {data_date} already exists. Use force_update=True to overwrite.\")\n",
    "            return 0\n",
    "        \n",
    "        self.logger.info(f\"Processing vol surfaces for date: {data_date}\")\n",
    "        \n",
    "        # Read Excel file\n",
    "        xl_file = pd.ExcelFile(excel_path)\n",
    "        sheet_names = xl_file.sheet_names\n",
    "        self.logger.info(f\"Found {len(sheet_names)} sheets\")\n",
    "        \n",
    "        # Connect to database\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create tables if they don't exist\n",
    "        self._create_tables(cursor)\n",
    "        \n",
    "        # Delete existing data for this date if force_update\n",
    "        if force_update and self.check_if_date_exists(data_date):\n",
    "            cursor.execute('DELETE FROM vol_surfaces WHERE data_date = ?', (data_date,))\n",
    "            cursor.execute('DELETE FROM surface_metadata WHERE data_date = ?', (data_date,))\n",
    "            self.logger.info(f\"Deleted existing data for {data_date}\")\n",
    "        \n",
    "        # Process each sheet\n",
    "        total_options = 0\n",
    "        for sheet_name in sheet_names:\n",
    "            count = self._process_sheet(cursor, xl_file, sheet_name, data_date)\n",
    "            total_options += count\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        self.logger.info(f\"Total options stored: {total_options}\")\n",
    "        \n",
    "        return total_options\n",
    "    \n",
    "    def _create_tables(self, cursor):\n",
    "        \"\"\"Create database tables with proper schema\"\"\"\n",
    "        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS vol_surfaces (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                data_date DATE NOT NULL,\n",
    "                sheet_name TEXT NOT NULL,\n",
    "                index_name TEXT NOT NULL,\n",
    "                tenor TEXT NOT NULL,\n",
    "                expiry DATE NOT NULL,\n",
    "                spot_level REAL,\n",
    "                forward_level REAL NOT NULL,\n",
    "                atm_strike REAL,\n",
    "                strike REAL NOT NULL,\n",
    "                option_type TEXT NOT NULL,\n",
    "                bid REAL,\n",
    "                ask REAL,\n",
    "                mid REAL,\n",
    "                delta REAL,\n",
    "                vol REAL NOT NULL,\n",
    "                change REAL,\n",
    "                breakeven REAL,\n",
    "                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "                UNIQUE(data_date, index_name, tenor, strike, option_type)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS surface_metadata (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                data_date DATE NOT NULL,\n",
    "                sheet_name TEXT NOT NULL,\n",
    "                index_name TEXT NOT NULL,\n",
    "                tenor TEXT NOT NULL,\n",
    "                expiry DATE NOT NULL,\n",
    "                spot_level REAL,\n",
    "                forward_level REAL NOT NULL,\n",
    "                atm_strike REAL,\n",
    "                update_time TEXT,\n",
    "                UNIQUE(data_date, sheet_name)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create indices for faster queries\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_vol_date ON vol_surfaces(data_date)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_vol_index ON vol_surfaces(index_name, tenor)')\n",
    "    \n",
    "    def _process_sheet(self, cursor, xl_file, sheet_name, data_date):\n",
    "        \"\"\"Process a single sheet and store data\"\"\"\n",
    "        \n",
    "        # Extract tenor from sheet name\n",
    "        tenor = self._extract_tenor(sheet_name)\n",
    "        if not tenor:\n",
    "            self.logger.warning(f\"Skipping {sheet_name} - cannot extract tenor\")\n",
    "            return 0\n",
    "        \n",
    "        # Read sheet data\n",
    "        df = xl_file.parse(sheet_name, header=None)\n",
    "        \n",
    "        # Parse the data\n",
    "        metadata, options = self._parse_sheet_data(df, sheet_name, tenor)\n",
    "        \n",
    "        if not metadata or not options:\n",
    "            self.logger.warning(f\"No valid data found in {sheet_name}\")\n",
    "            return 0\n",
    "        \n",
    "        # Add data_date\n",
    "        metadata['data_date'] = data_date\n",
    "        \n",
    "        # Store metadata\n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO surface_metadata \n",
    "            (data_date, sheet_name, index_name, tenor, expiry, \n",
    "             spot_level, forward_level, atm_strike, update_time)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            data_date,\n",
    "            sheet_name,\n",
    "            metadata['index_name'],\n",
    "            tenor,\n",
    "            metadata['expiry'],\n",
    "            metadata.get('spot_level'),\n",
    "            metadata['forward_level'],\n",
    "            metadata['atm_strike'],\n",
    "            metadata.get('update_time')\n",
    "        ))\n",
    "        \n",
    "        # Store options\n",
    "        for option in options:\n",
    "            cursor.execute('''\n",
    "                INSERT OR REPLACE INTO vol_surfaces \n",
    "                (data_date, sheet_name, index_name, tenor, expiry,\n",
    "                 spot_level, forward_level, atm_strike, strike,\n",
    "                 option_type, bid, ask, mid, delta, vol, change, breakeven)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                data_date,\n",
    "                sheet_name,\n",
    "                metadata['index_name'],\n",
    "                tenor,\n",
    "                metadata['expiry'],\n",
    "                metadata.get('spot_level'),\n",
    "                metadata['forward_level'],\n",
    "                metadata['atm_strike'],\n",
    "                option['strike'],\n",
    "                option['option_type'],\n",
    "                option.get('bid'),\n",
    "                option.get('ask'),\n",
    "                option.get('mid'),\n",
    "                option.get('delta'),\n",
    "                option['vol'],\n",
    "                option.get('change'),\n",
    "                option.get('breakeven')\n",
    "            ))\n",
    "        \n",
    "        self.logger.info(f\"  {sheet_name}: {len(options)} options stored\")\n",
    "        \n",
    "        return len(options)\n",
    "    \n",
    "    def _extract_tenor(self, sheet_name):\n",
    "        \"\"\"Extract tenor from sheet name\"\"\"\n",
    "        # Handle both '_' and '-' separators\n",
    "        if '_' in sheet_name:\n",
    "            parts = sheet_name.split('_')\n",
    "        elif '-' in sheet_name:\n",
    "            parts = sheet_name.split('-')\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        if len(parts) > 1:\n",
    "            tenor = parts[-1]\n",
    "            if re.match(r'^\\d+m$', tenor):\n",
    "                return tenor\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _parse_sheet_data(self, df, sheet_name, tenor):\n",
    "        \"\"\"Parse sheet data to extract metadata and options\"\"\"\n",
    "        \n",
    "        metadata = {'tenor': tenor}\n",
    "        options = []\n",
    "        \n",
    "        # Convert to lines for parsing\n",
    "        lines = []\n",
    "        for _, row in df.iterrows():\n",
    "            line = ' | '.join([str(cell) if pd.notna(cell) else '' for cell in row])\n",
    "            if line.strip():\n",
    "                lines.append(line)\n",
    "        \n",
    "        # Parse metadata from header lines\n",
    "        for line in lines[:5]:\n",
    "            # Update time\n",
    "            if 'Last updated:' in line:\n",
    "                match = re.search(r'(\\d{1,2}-\\w{3}-\\d{4})', line)\n",
    "                if match:\n",
    "                    metadata['update_time'] = match.group(1)\n",
    "            \n",
    "            # Index, expiry, forward, ATM\n",
    "            if 'Options:' in line:\n",
    "                # Determine index type\n",
    "                if 'MAIN' in line.upper():\n",
    "                    metadata['index_name'] = 'EU_IG'\n",
    "                elif 'XOVER' in line.upper() or 'XO' in line:\n",
    "                    metadata['index_name'] = 'EU_XO'\n",
    "                elif 'HY' in line:\n",
    "                    metadata['index_name'] = 'US_HY'\n",
    "                elif 'IG' in line:\n",
    "                    metadata['index_name'] = 'US_IG'\n",
    "                \n",
    "                # Extract expiry date\n",
    "                match = re.search(r'(\\d{1,2}-\\w{3}-\\d{2})', line)\n",
    "                if match:\n",
    "                    metadata['expiry'] = match.group(1)\n",
    "                \n",
    "                # Extract forward level\n",
    "                match = re.search(r'Fwd\\s*@([\\d.]+)', line)\n",
    "                if match:\n",
    "                    metadata['forward_level'] = float(match.group(1))\n",
    "                \n",
    "                # Extract ATM strike\n",
    "                match = re.search(r'Delta\\s*@([\\d.]+)', line)\n",
    "                if match:\n",
    "                    metadata['atm_strike'] = float(match.group(1))\n",
    "        \n",
    "        # Validate metadata\n",
    "        if 'index_name' not in metadata or 'forward_level' not in metadata:\n",
    "            return None, None\n",
    "        \n",
    "        # Parse option data lines\n",
    "        for line in lines:\n",
    "            if '|' not in line or ('Delta' in line and 'Vol' in line):\n",
    "                continue\n",
    "            \n",
    "            parts = line.split('|')\n",
    "            \n",
    "            # Process each option (Receiver and Payer pairs)\n",
    "            for i in range(0, len(parts), 2):\n",
    "                if i+1 >= len(parts):\n",
    "                    break\n",
    "                \n",
    "                strike_part = parts[i].strip()\n",
    "                data_part = parts[i+1].strip() if i+1 < len(parts) else ''\n",
    "                \n",
    "                # Extract strike\n",
    "                strike_match = re.search(r'([\\d.]+)', strike_part)\n",
    "                if not strike_match or not data_part:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    strike = float(strike_match.group(1))\n",
    "                    option_type = 'Receiver' if i == 0 else 'Payer'\n",
    "                    \n",
    "                    # Parse option details\n",
    "                    option = self._parse_option_details(data_part, strike, option_type)\n",
    "                    if option and option['vol']:\n",
    "                        options.append(option)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return metadata, options\n",
    "    \n",
    "    def _parse_option_details(self, data_str, strike, option_type):\n",
    "        \"\"\"Parse individual option data\"\"\"\n",
    "        \n",
    "        tokens = data_str.split()\n",
    "        if len(tokens) < 2:\n",
    "            return None\n",
    "        \n",
    "        option = {\n",
    "            'strike': strike,\n",
    "            'option_type': option_type,\n",
    "            'vol': None\n",
    "        }\n",
    "        \n",
    "        # Bid/Ask\n",
    "        if '/' in tokens[0]:\n",
    "            try:\n",
    "                parts = tokens[0].split('/')\n",
    "                option['bid'] = float(parts[0])\n",
    "                option['ask'] = float(parts[1])\n",
    "                option['mid'] = (option['bid'] + option['ask']) / 2\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Process remaining tokens\n",
    "        for i, token in enumerate(tokens[1:], 1):\n",
    "            if '%' in token:\n",
    "                try:\n",
    "                    option['delta'] = float(token.rstrip('%'))\n",
    "                except:\n",
    "                    pass\n",
    "            elif i == 2 and re.match(r'^[\\d.]+$', token):\n",
    "                option['vol'] = float(token)\n",
    "            elif i == 3 and re.match(r'^-?[\\d.]+$', token):\n",
    "                option['change'] = float(token)\n",
    "            elif i == 4 and re.match(r'^[\\d.]+$', token):\n",
    "                option['breakeven'] = float(token)\n",
    "        \n",
    "        return option if option['vol'] else None\n",
    "    \n",
    "    def query_surface(self, index_name=None, tenor=None, data_date=None):\n",
    "        \"\"\"Query vol surface data\"\"\"\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        query = \"SELECT * FROM vol_surfaces WHERE 1=1\"\n",
    "        params = []\n",
    "        \n",
    "        if index_name:\n",
    "            query += \" AND index_name = ?\"\n",
    "            params.append(index_name)\n",
    "        \n",
    "        if tenor:\n",
    "            query += \" AND tenor = ?\"\n",
    "            params.append(tenor)\n",
    "        \n",
    "        if data_date:\n",
    "            query += \" AND data_date = ?\"\n",
    "            params.append(data_date)\n",
    "        \n",
    "        query += \" ORDER BY strike, option_type\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn, params=params)\n",
    "        conn.close()\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Daily update script (save as separate file: update_vol_surfaces.py)\n",
    "def daily_update():\n",
    "    \"\"\"\n",
    "    Daily update script - run via Windows Task Scheduler\n",
    "    \"\"\"\n",
    "    vol_db = VolSurfaceDatabase()\n",
    "    \n",
    "    excel_path = r\"C:\\Users\\alessandro.esposito\\Portman Square Capital LLP\\Portman Square Capital - Documents\\S\\CSA\\Credit Index Trading\\vol_db.xlsx\"\n",
    "    \n",
    "    today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    if not vol_db.check_if_date_exists(today):\n",
    "        total = vol_db.create_or_update_database(excel_path, data_date=today)\n",
    "        print(f\"Updated {total} options for {today}\")\n",
    "    else:\n",
    "        print(f\"Data for {today} already exists\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    vol_db = VolSurfaceDatabase()\n",
    "    \n",
    "    excel_path = r\"C:\\Users\\alessandro.esposito\\Portman Square Capital LLP\\Portman Square Capital - Documents\\S\\CSA\\Credit Index Trading\\vol_db.xlsx\"\n",
    "    \n",
    "    today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    if not vol_db.check_if_date_exists(today):\n",
    "        total = vol_db.create_or_update_database(excel_path, data_date=today)\n",
    "        print(f\"Processed {total} options for {today}\")\n",
    "    else:\n",
    "        print(f\"Data for {today} already exists. Use force_update=True to overwrite.\")\n",
    "    \n",
    "    # Windows Task Scheduler setup instructions\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"To set up automated daily updates:\")\n",
    "    print(\"1. Save the daily_update() function as 'update_vol_surfaces.py'\")\n",
    "    print(\"2. Open Windows Task Scheduler\")\n",
    "    print(\"3. Create Basic Task > Daily > Set time (e.g., 8:00 AM)\")\n",
    "    print(\"4. Action: Start a program\")\n",
    "    print(\"5. Program: python.exe\")\n",
    "    print(\"6. Arguments: update_vol_surfaces.py\")\n",
    "    print(\"7. Start in: [directory of the script]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a12408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No database found at source (vol_surfaces.db) or target (C:\\source\\repos\\psc\\packages\\psc_csa_tools\\credit_macro\\data\\raw\\jpm_vol_surfaces.db)\n",
      "Please run the vol surface parser first to create the database\n",
      "Failed to create or access database at: C:\\source\\repos\\psc\\packages\\psc_csa_tools\\credit_macro\\data\\raw\\jpm_vol_surfaces.db\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "source_db = \"vol_surfaces.db\"\n",
    "target_dir = r\"C:\\source\\repos\\psc\\packages\\psc_csa_tools\\credit_macro\\data\\raw\"\n",
    "target_db = os.path.join(target_dir, \"jpm_vol_surfaces.db\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Check if target database already exists\n",
    "if os.path.exists(target_db):\n",
    "    print(f\"Database already exists at: {target_db}\")\n",
    "    use_existing = True\n",
    "else:\n",
    "    # Check if source database exists to copy\n",
    "    if os.path.exists(source_db):\n",
    "        shutil.copy2(source_db, target_db)\n",
    "        print(f\"Database copied to: {target_db}\")\n",
    "        use_existing = False\n",
    "    else:\n",
    "        print(f\"No database found at source ({source_db}) or target ({target_db})\")\n",
    "        print(\"Please run the vol surface parser first to create the database\")\n",
    "        exit()\n",
    "\n",
    "# Connect to the database and examine schema\n",
    "if os.path.exists(target_db):\n",
    "    conn = sqlite3.connect(target_db)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATABASE SCHEMA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get all tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = cursor.fetchall()\n",
    "    \n",
    "    if not tables:\n",
    "        print(\"Database exists but contains no tables. Run the parser to populate it.\")\n",
    "        conn.close()\n",
    "        exit()\n",
    "    \n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        print(f\"\\nTable: {table_name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Get table structure\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        columns = cursor.fetchall()\n",
    "        \n",
    "        # Format column info\n",
    "        for col in columns:\n",
    "            col_id, name, dtype, notnull, default, pk = col\n",
    "            pk_text = \" [PRIMARY KEY]\" if pk else \"\"\n",
    "            null_text = \" NOT NULL\" if notnull else \"\"\n",
    "            default_text = f\" DEFAULT {default}\" if default else \"\"\n",
    "            print(f\"  {name:20} {dtype:10}{pk_text}{null_text}{default_text}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATABASE CONTENTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Count records in each table\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"{table_name}: {count} records\")\n",
    "    \n",
    "    # Check if vol_surfaces table exists and has data\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='vol_surfaces'\")\n",
    "    if cursor.fetchone():\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM vol_surfaces\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        \n",
    "        if count > 0:\n",
    "            # Show sample data structure\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VOL_SURFACES TABLE - DATA STRUCTURE\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            sample_query = \"\"\"\n",
    "            SELECT \n",
    "                sheet_name,\n",
    "                index_name,\n",
    "                tenor,\n",
    "                expiry,\n",
    "                forward_level,\n",
    "                strike,\n",
    "                option_type,\n",
    "                bid,\n",
    "                ask,\n",
    "                delta,\n",
    "                vol\n",
    "            FROM vol_surfaces\n",
    "            LIMIT 5\n",
    "            \"\"\"\n",
    "            \n",
    "            df_sample = pd.read_sql_query(sample_query, conn)\n",
    "            print(df_sample.to_string())\n",
    "            \n",
    "            # Show unique values for key fields\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"UNIQUE VALUES IN KEY FIELDS\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Unique indices\n",
    "            cursor.execute(\"SELECT DISTINCT index_name FROM vol_surfaces ORDER BY index_name\")\n",
    "            indices = [row[0] for row in cursor.fetchall()]\n",
    "            print(f\"\\nIndices: {indices}\")\n",
    "            \n",
    "            # Unique expiries\n",
    "            cursor.execute(\"SELECT DISTINCT expiry FROM vol_surfaces ORDER BY expiry\")\n",
    "            expiries = [row[0] for row in cursor.fetchall()]\n",
    "            print(f\"\\nExpiries: {expiries}\")\n",
    "            \n",
    "            # Unique tenors\n",
    "            cursor.execute(\"SELECT DISTINCT tenor FROM vol_surfaces ORDER BY tenor\")\n",
    "            tenors = [row[0] for row in cursor.fetchall()]\n",
    "            print(f\"\\nTenors: {tenors}\")\n",
    "            \n",
    "            # Check for data dates if column exists\n",
    "            cursor.execute(\"PRAGMA table_info(vol_surfaces)\")\n",
    "            columns = [col[1] for col in cursor.fetchall()]\n",
    "            if 'data_date' in columns:\n",
    "                cursor.execute(\"SELECT DISTINCT data_date FROM vol_surfaces ORDER BY data_date DESC\")\n",
    "                dates = [row[0] for row in cursor.fetchall()]\n",
    "                print(f\"\\nData dates: {dates[:5]}\")  # Show first 5 dates\n",
    "            \n",
    "            # Strike ranges by index\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"STRIKE RANGES BY INDEX\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            strike_ranges = pd.read_sql_query(\"\"\"\n",
    "                SELECT \n",
    "                    index_name,\n",
    "                    MIN(strike) as min_strike,\n",
    "                    MAX(strike) as max_strike,\n",
    "                    COUNT(DISTINCT strike) as num_strikes\n",
    "                FROM vol_surfaces\n",
    "                GROUP BY index_name\n",
    "                ORDER BY index_name\n",
    "            \"\"\", conn)\n",
    "            \n",
    "            print(strike_ranges.to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\nvol_surfaces table exists but is empty. Run the parser to populate it.\")\n",
    "    else:\n",
    "        print(\"\\nvol_surfaces table does not exist. Run the parser to create it.\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"\\nâœ“ Database is ready at: {target_db}\")\n",
    "else:\n",
    "    print(f\"Failed to create or access database at: {target_db}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
